---
layout: post
title: 阅读笔记 YOLO9000: Better, Faster, Stronger
---
论文链接 [YOLO9000: Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

YOLOv2是对YOLO的升级，使用multi-scale training method，它可以适应不同输入尺寸，因此也更容易对速度和精度做权衡。
在VOC2007数据集上 76.8mAP 67FPS, 78.6mAP 40FPS, 性能优于ResNet FRCNN和SSD。
tiny model在TITAN X可以达到200FPS的速度。

代码和预训练模型下载(https://pjreddie.com/darknet/yolo/)

##Better
与FRCNN相比，YOLO的定位精度很差。并且与region proposal的方法相比，其召回要低很多。因此优化集中在保证分类准确率的前提下，提高准确率和定位精度。
![_config.yml]({{ site.baseurl }}/images/yolo9000/yolov2_optimization.png)

#Batch Normalization: Batch Normalization显著提高了网络的收敛速度，并且可以去掉其他的正则化方法。添加Batch Normalization，mAP提升2%，并且去掉dropout后不会出现过拟合。

#High Resolution Classifier: 原始的YOLO训练分类模型的时候输入图像尺寸为224x224，检测时输入尺寸变为448x448。这样就会导致在训练检测模型的同时还要学习新分辨率的变化。
因此YOLOv2的分类模型先在ImageNet数据集上，使用448x448的图像finetuning 10epochs，这样就会消除输入图像尺寸带来的影响。然后再fine tuning训练检测模型。
fine tune高分辨率的分类模型收益是提高4%mAP。

#Convolutional With Anchor Boxes: YOLO是直接在最后提取的卷积特征后添加了一层全连接来实现bounding box坐标的预测。
与直接YOLO直接预测坐标的方式不同，FRCNN使用手工设计的经验值来预测bounding box。使用RPN来预测anchor box的偏移和置信度。预测偏移量，而不直接预测坐标值的好处是网络更容易学习。
因此YOLOv2的修改策略是去掉YOLO的全连接层，添加anchor boxes来预测bounding box。
1. 消除一个pooling层，使得网络中卷积输出的feature map具有更高的分辨率。
2. 修改网络的输入，由448x448改为416x416。这样做的好处是feature map是奇数，只有一个中心。
因为作者观察到，大物体通常占据了图像的中间位置，可以只用一个中心的cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。
YOLO卷积层的下采样因子为32，因此输入为416得到的feature map尺寸为13x13。
3. 类别预测机制从空间定位中解耦出来。YOLO是用每个cell来预测类别，YOLOv2是把类别跟anchor绑定在一起。参考如下图很容易理解。
使用anchor后在准确率方面会有所下降。原始YOLO是7x7cell，每个cell对应两个bounding box，因此每幅图像仅预测49个bounding box。但是使用anchor后每幅图像有超过1000个bounding box。
不使用anchor准确率为69.5mAP，召回率为81%。使用anchor后，准确率69.2mAP，召回率88%。虽然准确率下降一些，但是召回率有明显的提升。
![_config.yml]({{ site.baseurl }}/images/yolo9000/yolo_vs_yolov2.png)
(盗图自：https://zhuanlan.zhihu.com/p/25167153?1487044896845)

#Dimension Clusters: YOLO添加anchor box时发现了两个问题，第一个问题就是如果人工设计anchor box，初始anchor设计的好，网络更容易收敛，同时检测效果也会更好。
为了获取更好的anchor boxes，用k-means聚类来选择合适的priors。如果是欧式距离来聚类，窗口越大产生的误差也就越大。
然而，我们希望priors与IOU分数相关，与窗口的尺寸无关，因此使用如下公式来度量聚类距离：
$$d(box,centroid) = 1 - IOU(box,centroid)$$
![_config.yml]({{ site.baseurl }}/images/yolo9000/dimension_clusters.png)
上图为变量$k$和平均IOU的分布曲线，权衡模型的复杂度和召回情况，最终$k$设置为5。聚类中心与手工设计的anchor boxes有显著的不同，短宽的box较少，瘦高的box更多。
如下图所示为聚类anchor和手工anchor的比较。使用k-means聚类生成bounding box，使得网络开始有更好的表达，也是模型更容易训练。
![_config.yml]({{ site.baseurl }}/images/yolo9000/hand_vs_cluster.png)

#Direct location prediction: YOLO添加anchor box时遇到的第二个问题是模型的不稳定，尤其是在早期迭代的时候。主要的不稳定因素来自预测box的位置(x,y)。
在region proposal networks中，网络预测$t_{x}$和$t_{y}$，因此中心坐标$(x,y)$可以表示为：
$$x=(t_{x}*w_{a}) + x_{a}$$
$$y=(t_{y}*h_{a}) + y_{a}$$
这个公式没有任何限制，因此不论预测的窗口在哪，anchor box可能在图像的任意位置。模型随机初始化后，需要很长时间才能稳定地预测敏感的物体偏移。
由于模型初始会不稳定这个问题，YOLOv2没有像FRCNN那样预测窗口的偏移值。YOLOv2还是沿用YOLO预测相对于grid cell的坐标，并且将真值限定在0到1的范围内。添加了一个logistic activation来限制网络的输出范围。
在最后的feature map上，每个cell预测5个bounding boxes。每个bounding box预测5个坐标值$t_{x}$，$t_{y}$，$t_{w}$，$t_{h}$和$t_{o}$。
左上角为图像的坐标原点，cell的偏移坐标为$(c_{x},c_{y})$，bounding box prior的宽高为$p_{w}$，$p_{h}$，网络预测为：
$$b_{x}=\sigma(t_{x}) + c_{x}$$
$$b_{y}=\sigma(t_{y}) + c_{y}$$
$$b_{w}=p_{w}e^{t_{w}}$$
$$b_{h}=p_{h}e^{t_{h}}$$
$$P_{r}(object)*IOU(b,object)=\sigma(t_{o})$$
添加了位置预测的限定，参数更容易训练，模型也更稳定。使用dimension clusters及direct location prediction两种anchor改进方法，提高5%mAP。
![_config.yml]({{ site.baseurl }}/images/yolo9000/bounding_boxes.png)

#Fine-Grained Features: 从高分辨率feature到底分辨率feature添加连通路，与ResNet的identity mappings类似，将相邻特征叠加成不同的通道，而不是空间位置。
实际操作将前面的26x26的feature map与后面相连。实现方式现将feature变换成13x13，如$26x26x512 -> 13x13x2048$，然后再与后面的13x13 feature map合并channel，最后在这个合并的feature map上做检测。
这样合并是为了提高小物体的检测，至少有1%mAP的提升。

#Multi-Scale Training: 原始YOLO是输入分辨率为448x448，添加anchor boxes后分辨率变为416x416。由于网络中只有卷积和pooling层，对输入尺寸没有限制。
我们希望YOLOv2能够对不同输入尺寸都具有鲁棒性，将尺度信息训练到模型中。方法是不再固定输入尺寸，每10个batch随机选取一个新的图像尺寸。
由于模型的下采样因子是32，设置的模型尺寸为${320,352,...,608}$，因此最小的选择是320x320，最大尺寸为608x608。将网络resize到对应维数，然后继续训练。
这种策略能让模型学习多种输入尺寸的预测，意味着同一个网络可以适应多种分辨率的输入。输入尺寸越小，速度越快，YOLOv2可以很好地权衡速度和准确率。
低分辨率的情况下，如288x288 YOLOv2可以达到90FPS，并且mAP几乎与Fast R-CNN相同。
在高分辨率的情况下，YOLOv2是state-of-the-art检测器，在VOC2007上达到78.6mAP，仍然可以达到实时的速度。下图是不同框架在VOC 2007数据集上的评测对比。
![_config.yml]({{ site.baseurl }}/images/yolo9000/voc2007_accuracy_speed.png)
![_config.yml]({{ site.baseurl }}/images/yolo9000/voc2007_bunchmark.png)


##Faster


reference:
http://blog.csdn.net/jesse_mx/article/details/53925356
https://zhuanlan.zhihu.com/p/25167153?1487044896845
https://zhuanlan.zhihu.com/p/24916786?refer=xiaoleimlnote
