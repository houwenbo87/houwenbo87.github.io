---
layout: post
title: 阅读笔记 YOLO9000 Better, Faster, Stronger
---
论文链接 [YOLO9000: Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
## Abstract

YOLOv2是对YOLO的升级，使用multi-scale training method，它可以适应不同输入尺寸，因此也更容易对速度和精度做权衡。
在VOC2007数据集上 76.8mAP 67FPS, 78.6mAP 40FPS, 性能优于ResNet FRCNN和SSD。
tiny model在TITAN X可以达到200FPS的速度。

代码和预训练模型下载(https://pjreddie.com/darknet/yolo/)

## Better

与FRCNN相比，YOLO的定位精度很差。并且与region proposal的方法相比，其召回要低很多。因此优化集中在保证分类准确率的前提下，提高准确率和定位精度。如下图为使用的优化策略及对应的收益。
![_config.yml]({{ site.baseurl }}/images/yolo9000/yolov2_optimization.png)

### Batch Normalization
Batch Normalization显著提高了网络的收敛速度，并且可以去掉其他的正则化方法。
添加Batch Normalization，mAP提升2%，并且去掉dropout后不会出现过拟合。

### High Resolution Classifier
原始的YOLO训练分类模型的时候输入图像尺寸为224x224，检测时输入尺寸变为448x448。这样就会导致在训练检测模型的同时还要学习新分辨率的变化。
因此YOLOv2的分类模型先在ImageNet数据集上，使用448x448的图像finetuning 10epochs，这样就会消除输入图像尺寸带来的影响。然后再fine tuning训练检测模型。
fine tune高分辨率的分类模型收益是提高4%mAP。

### Convolutional With Anchor Boxes
YOLO是直接在最后提取的卷积特征后添加了一层全连接来实现bounding box坐标的预测。
与直接YOLO直接预测坐标的方式不同，FRCNN使用手工设计的经验值来预测bounding box。使用RPN来预测anchor box的偏移和置信度。预测偏移量，而不直接预测坐标值的好处是网络更容易学习。
因此YOLOv2的修改策略是去掉YOLO的全连接层，添加anchor boxes来预测bounding box。
1. 消除一个pooling层，使得网络中卷积输出的feature map具有更高的分辨率。

2. 修改网络的输入，由448x448改为416x416。这样做的好处是feature map是奇数，只有一个中心。
因为作者观察到，大物体通常占据了图像的中间位置，可以只用一个中心的cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。
YOLO卷积层的下采样因子为32，因此输入为416得到的feature map尺寸为13x13。

3. 类别预测机制从空间定位中解耦出来。YOLO是用每个cell来预测类别，YOLOv2是把类别跟anchor绑定在一起。参考如下图很容易理解。

![_config.yml]({{ site.baseurl }}/images/yolo9000/yolo_vs_yolov2.png)

(盗图自：https://zhuanlan.zhihu.com/p/25167153?1487044896845)

使用anchor后在准确率方面会有所下降。原始YOLO是7x7cell，每个cell对应2个bounding box，因此每幅图像仅预测49个bounding box。但是使用anchor后每幅图像有超过1000个bounding box。
不使用anchor准确率为69.5mAP，召回率为81%。使用anchor后，准确率69.2mAP，召回率88%。虽然准确率下降一些，但是召回率有明显的提升。

YOLO添加Anchor Box之后遇到两个问题，详见下面Dimension Clusters和Direct location prediction。

### Dimension Clusters
YOLO添加anchor box时发现了两个问题，第一个问题就是如果人工设计anchor box，初始anchor设计的好，网络更容易收敛，同时检测效果也会更好。

为了获取更好的anchor boxes，用k-means聚类来选择合适的priors。如果是欧式距离来聚类，窗口越大产生的误差也就越大。
然而，我们希望priors与IOU相关，与窗口的尺寸无关，因此使用如下公式来度量聚类距离：

$$d(box,centroid) = 1 - IOU(box,centroid)$$

![_config.yml]({{ site.baseurl }}/images/yolo9000/dimension_clusters.png)

上图为变量\\(k\\)和平均IOU的分布曲线，权衡模型的复杂度和召回情况，最终\\(k\\)设置为5。聚类中心与手工设计的anchor boxes有显著的不同，短宽的box较少，瘦高的box更多。
如下图所示为聚类anchor和手工anchor的比较。使用k-means聚类生成bounding box，使得网络开始有更好的表达，也是模型更容易训练。
![_config.yml]({{ site.baseurl }}/images/yolo9000/hand_vs_cluster.png)

### Direct location prediction
YOLO添加anchor box时遇到的第二个问题是模型的不稳定，尤其是在早期迭代的时候。主要的不稳定因素来自预测box的位置(x,y)。
在RPN网络中，网络预测\\(t_{x}\\)和\\(t_{y}\\)，因此中心坐标\\((x,y)\\)表示为：

$$x=(t_{x}*w_{a}) + x_{a}$$

$$y=(t_{y}*h_{a}) + y_{a}$$

这个公式没有任何约束限制，因此不论预测的窗口在哪，anchor box可能在图像的任意位置。模型随机初始化后，需要很长时间才能稳定地预测敏感的物体偏移。

由于模型初始会不稳定这个问题，YOLOv2没有像FRCNN那样预测窗口的偏移值。YOLOv2还是沿用YOLO预测相对于grid cell的坐标，并且将真值限定在0到1的范围内。添加了一个logistic activation来限制网络的输出范围。

在最后的feature map上，每个cell预测5个bounding boxes。每个bounding box预测5个坐标值\\(t_{x}\\)，\\(t_{y}\\)，\\(t_{w}\\)，\\(t_{h}\\)和\\(t_{o}\\)。
左上角为图像的坐标原点，cell的偏移坐标为\\((c_{x},c_{y})\\)，bounding box prior的宽高为\\(p_{w}\\)，\\(p_{h}\\)，网络预测为：

$$b_{x}=\sigma(t_{x}) + c_{x}$$

$$b_{y}=\sigma(t_{y}) + c_{y}$$

$$b_{w}=p_{w}e^{t_{w}}$$

$$b_{h}=p_{h}e^{t_{h}}$$

$$P_{r}(object)*IOU(b,object)=\sigma(t_{o})$$

添加了位置预测的限定，参数更容易训练，模型也更稳定。使用dimension clusters及direct location prediction两种anchor改进方法，提高5%mAP。
![_config.yml]({{ site.baseurl }}/images/yolo9000/bounding_boxes.png)

### Fine-Grained Features
从高分辨率feature到低分辨率feature添加连通路，与ResNet的identity mappings类似，将相邻特征叠加成不同的通道，而不是空间位置。
实际操作将前面的26x26的feature map与后面相连。实现方式现将feature变换成13x13，如26x26x512 -> 13x13x2048，然后再与后面的13x13 feature map合并channel，最后在这个合并的feature map上做检测。
这样合并是为了提高小物体的检测，至少有1%mAP的提升。

### Multi-Scale Training
原始YOLO是输入分辨率为448x448，添加anchor boxes后分辨率变为416x416。由于网络中只有卷积和pooling层，对输入尺寸没有限制。

我们希望YOLOv2能够对不同输入尺寸都具有鲁棒性，将尺度信息训练到模型中。方法是不再固定输入尺寸，每10个batch随机选取一个新的图像尺寸。
由于模型的下采样因子是32，设置的模型尺寸为\\(\{320,352,...,608\}\\)，因此最小的选择是320x320，最大尺寸为608x608。将网络resize到对应维数，然后继续训练。

这种策略能让模型学习多种输入尺寸的预测，意味着同一个网络可以适应多种分辨率的输入。输入尺寸越小，速度越快，YOLOv2可以很好地权衡速度和准确率。

低分辨率的情况下，如288x288 YOLOv2可以达到90FPS，并且mAP几乎与Fast R-CNN相同。

在高分辨率的情况下，YOLOv2是state-of-the-art检测器，在VOC2007上达到78.6mAP，仍然可以达到实时的速度。下图是不同框架在VOC 2007数据集上的评测对比。
![_config.yml]({{ site.baseurl }}/images/yolo9000/voc2007_accuracy_speed.png)
![_config.yml]({{ site.baseurl }}/images/yolo9000/voc2007_bunchmark.png)


## Faster
大部分检测框架依赖VGG16作为基础特征提取网络。虽然VGG16是一个强大而准确的网络，但是过于复杂，参数过多。输入一幅224x224的图像，它需要30.69billion次浮点运算。

YOLO框架使用类似GoogleNet结构的网络，仅需要8.52 billion次浮点运算，并且准确率稍稍低于VGG16. YOLO的分类模型在ImageNet数据上88.0%mAP，VGG16是90%。

### Darknet-19
提出一个新的分类模型作为YOLOv2的特征提取部分。参考了前人的工作经验。类似于VGG，网络使用了较多的3 * 3卷积核，在每一次池化操作后把通道数翻倍。借鉴了network in network的思想，网络使用了全局平均池化（global average pooling）做预测，把1 * 1的卷积核置于3 * 3的卷积核之间，用来压缩特征。使用batch normalization稳定模型训练，加速收敛，正则化模型。

最终得出的基础模型就是Darknet-19，包含19个卷积层，5个最大值池化层(max pooling layers)。Darknet-19处理一张照片需要55.8亿次运算，imagenet的top-1准确率为72.9%，top-5准确率为91.2%。

在K40显卡上进行对比，GoogleNetv1的forward耗时42.32ms，Darknet-19的forward耗时17.50ms。

![_config.yml]({{ site.baseurl }}/images/yolo9000/darknet-19.png)

### Training for classification
使用标准的ImageNet 1000类数据训练分类网络，迭代160 epochs，使用随机梯度下降，初始学习率为0.1，多项式下降参数为4，weight decay 为0.0005，冲量为0.9.

使用了标准的数据增强算法，包括随机裁剪、旋转、饱和度、色度、曝光偏移等。

正如上面讨论的，初始训练使用224x224的图像，然后使用448x448的尺寸finetune模型。使用上面的参数训练10 epochs，初始学习率为\\(10^(-3)\\)。在高分辨率的情况下，top-1准确率为76.5%，top-5准确率为93.3%。

### Training for detection
为了实现检测，对Darknet-19做了一定的修改：去掉最后的卷积层，增加3个有1024 filter的3x3卷积，并且每一个卷积层后面添加一个1x1的卷积，输出维度对应实际的输出数量。

对于VOC数据集，我们预测5种boxes，每种boxes有5个坐标和20个类别，所以一共是\\(5x(5+20)=125\\)维输出。

也添加了passthrough layer，从最后3 * 3 * 512的卷积层连到倒数第二层，使模型有了细粒度特征。

学习策略是：先以\\(10^{-3}\\)的初始学习率训练了160次，在第60次和第90次的时候学习率减为原来的十分之一。weight decay为0.0005，momentum为0.9，以及类似于Faster-RCNN和SSD的数据扩充（data augmentation）策略： random crops, color shifting, etc。使用相同的策略在 COCO 和VOC上训练。

下面的Stronger部分，介绍如何在YOLOv2的基础上实现YOLO9000.

## Stronger

待补充~~

## reference:
http://blog.csdn.net/jesse_mx/article/details/53925356
https://zhuanlan.zhihu.com/p/25167153?1487044896845
https://zhuanlan.zhihu.com/p/24916786?refer=xiaoleimlnote
